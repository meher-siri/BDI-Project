# -*- coding: utf-8 -*-
"""collaborative_filtering_ALS_Implementation_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RMLfq0nh8NqrlOfifnSjVoVLXyUFZaEZ

# ALS Collaborative Filtering
"""

!pip install pyspark
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import col, expr, explode
from pyspark.ml.recommendation import ALS, ALSModel
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.feature import StringIndexer
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
import numpy as np

from pyspark.sql.functions import col, expr, collect_list, explode
from pyspark.sql.types import StructType, StructField, IntegerType, StringType
from pyspark.sql import functions as F

from pyspark.sql import SparkSession
from pyspark.ml.recommendation import ALS, ALSModel
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import col, expr, unix_timestamp, from_unixtime, explode, collect_list, size, sum as _sum, array_contains
from pyspark.sql.types import StructType, StructField, LongType, IntegerType, StringType
from pyspark.sql.window import Window

class ALSCollabFilter(object):
    def __init__(self, file_path: str, k=10, model_dir=None):
        self.k = k
        self.model_dir = model_dir
        self.spark = SparkSession.builder.appName("ALSCollabFilterV3") \
                                 .config("spark.driver.memory", "8g") \
                                 .config("spark.executor.memory", "8g") \
                                 .getOrCreate()
        self.file_path = file_path
        self.data = self.__load_data()
        self.__prepare_data()
        self.training_data, self.test_data = self.__split_data()
        if self.training_data is not None and self.test_data is not None:
            self.__train_and_save_model()
            self.__evaluate_model()

    def __load_data(self):
        schema = StructType([
            StructField("timestamp", LongType()),
            StructField("visitorid", IntegerType()),
            StructField("event", StringType()),
            StructField("itemid", IntegerType()),
            StructField("transactionid", IntegerType())
        ])
        return self.spark.read.csv(self.file_path, header=True, schema=schema)

    def __prepare_data(self):
        self.data = self.data.withColumn("timestamp", (col("timestamp") / 1000).cast('timestamp'))
        self.data = self.data.na.drop(subset=["visitorid", "itemid"])
        self.data = self.data.dropDuplicates(["visitorid", "itemid", "event"])
        self.data = self.data.withColumnRenamed("visitorid", "userId") \
                             .withColumnRenamed("itemid", "itemId") \
                             .withColumn("rating", expr("case when event='view' then 1 when event='addtocart' then 2 when event='transaction' then 3 else 0 end"))

    def __split_data(self):
        self.data = self.data.withColumn("timestamp", unix_timestamp(col("timestamp")).cast('long'))
        quantile = self.data.stat.approxQuantile("timestamp", [0.8], 0.05)
        if not quantile:
            return None, None
        return self.data.filter(col("timestamp") < quantile[0]), self.data.filter(col("timestamp") >= quantile[0])

    def __train_and_save_model(self):
        als = ALS(maxIter=5, regParam=0.1, userCol="userId", itemCol="itemId", ratingCol="rating", implicitPrefs=False, coldStartStrategy="drop")
        self.model = als.fit(self.training_data)
        if self.model_dir:
            self.model.write().overwrite().save(self.model_dir)

    def __evaluate_model(self):
        predictions = self.model.transform(self.test_data)
        evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating", predictionCol="prediction")
        rmse = evaluator.evaluate(predictions)
        print(f"RMSE: {rmse}")

    def get_recommendations_for_user(self, userId, top_k=10):
        user_df = self.spark.createDataFrame([(userId,)], ["userId"])
        recommendations = self.model.recommendForUserSubset(user_df, top_k)
        recommendations = recommendations.withColumn("recommendations", explode("recommendations"))
        recommendations = recommendations.select("userId", col("recommendations.itemId").alias("itemId"), col("recommendations.rating").alias("rating"))
        return recommendations

model = ALSCollabFilter(file_path="/content/events.csv", model_dir="/content/model")

recommendations = model.get_recommendations_for_user(userId='992329', top_k=10)
recommendations.show()

import streamlit as st
from pyspark.sql import SparkSession
from pyspark.ml.recommendation import ALSModel

# Define the path to the model directory
model_dir = "/content/model"

# Initialize the Spark session
spark = SparkSession.builder.appName("StreamlitALSDemo") \
    .config("spark.driver.memory", "8g") \
    .config("spark.executor.memory", "8g") \
    .getOrCreate()

# Load the trained model
model = ALSModel.load(model_dir)

def get_recommendations_for_user(userId, top_k=10):
    user_df = spark.createDataFrame([(userId,)], ["userId"])
    recommendations = model.get_recommendations_for_user(user_df, top_k)
    recommendations = recommendations.withColumn("recommendations", explode("recommendations"))
    recommendations = recommendations.select("userId", col("recommendations.itemId").alias("itemId"), col("recommendations.rating").alias("rating"))
    return recommendations

# Streamlit UI
st.title('ALS Recommendations')
user_input = st.text_input("Enter User ID", '992329')
top_k = st.slider('Number of Recommendations', min_value=1, max_value=20, value=10)
if st.button('Get Recommendations'):
    try:
        user_id = int(user_input)
        recommendations_df = get_recommendations_for_user(user_id, top_k=top_k)
        recommendations_pandas = recommendations_df.toPandas()
        st.write('Recommendations:')
        st.dataframe(recommendations_pandas)
    except Exception as e:
        st.error(f"Error: {str(e)}")